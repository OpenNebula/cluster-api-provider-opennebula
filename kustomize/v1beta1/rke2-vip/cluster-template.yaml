---
apiVersion: v1
kind: Secret
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
stringData:
  ONE_XMLRPC: "${ONE_XMLRPC}"
  ONE_AUTH: "${ONE_AUTH}"
type: Opaque
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONEMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-cp"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      templateName: "${MASTER_TEMPLATE_NAME}"
---
#https://github.com/rancher/cluster-api-provider-rke2/blob/main/controlplane/api/v1beta1/rke2controlplane_types.go#L46
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: RKE2ControlPlane
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
  registrationMethod: "address"
  registrationAddress: "${CONTROL_PLANE_HOST}"
  files:
    - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
      owner: "root:root"
      permissions: "0644"
      content: |
        ---
        apiVersion: v1
        kind: Namespace
        metadata:
          name: metallb-system
        ---
        apiVersion: helm.cattle.io/v1
        kind: HelmChart
        metadata:
          name: metallb
          namespace: metallb-system
        spec:
          bootstrap: true
          chart: "metallb"
          repo: "https://metallb.github.io/metallb"
          version: "0.14.9"
          targetNamespace: metallb-system
          valuesContent: |-
            loadBalancerClass: "metallb"
            controller:
              nodeSelector:
                node-role.kubernetes.io/control-plane: "true"
              tolerations:
              - key: node.cloudprovider.kubernetes.io/uninitialized
                value: "true"
                effect: NoSchedule
              - key: node-role.kubernetes.io/etcd
                effect: NoExecute
              - key: node-role.kubernetes.io/master
                effect: NoSchedule
              - key: node-role.kubernetes.io/control-plane
                effect: NoSchedule
            speaker:
              frr: { enabled: false }
              tolerations:
              - key: node.cloudprovider.kubernetes.io/uninitialized
                value: "true"
                effect: NoSchedule
              - key: node-role.kubernetes.io/etcd
                effect: NoExecute
              - key: node-role.kubernetes.io/master
                effect: NoSchedule
              - key: node-role.kubernetes.io/control-plane
                effect: NoSchedule
    - path: /var/lib/rancher/rke2/server/manifests/metallb-resources.yaml
      owner: "root:root"
      permissions: "0644"
      content: |
        ---
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: one
          namespace: metallb-system
        spec:
          addresses:
            - "${CONTROL_PLANE_HOST}/32"
        ---
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: l2advertisement
          namespace: metallb-system
        spec:
          ipAddressPools: [one]
          nodeSelectors:
            - matchLabels:
                node-role.kubernetes.io/control-plane: "true"
    - path: /var/lib/rancher/rke2/server/manifests/cluster-vip.yaml
      owner: "root:root"
      permissions: "0644"
      content: |
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: cluster-vip
          namespace: kube-system
          annotations:
            metallb.io/allow-shared-ip: cluster-external-ip
            metallb.io/loadBalancerIPs: "${CONTROL_PLANE_HOST}"
        spec:
          type: LoadBalancer
          loadBalancerClass: "metallb"
          ports:
            - name: rke2-proxy
              port: 54321
              protocol: TCP
              targetPort: 54321
          selector:
            component: kube-apiserver
    - path: /etc/systemd/system/cluster-vip.service
      owner: "root:root"
      permissions: "0644"
      content: |
        [Unit]
        After=network.target
        [Service]
        ExecStart=/usr/local/bin/cluster-vip.sh
        [Install]
        WantedBy=multi-user.target
    - path: /usr/local/bin/cluster-vip.sh
      owner: "root:root"
      permissions: "0755"
      content: |
        #!/usr/bin/env sh
        set -xeu
        ipt_nat_prerouting_prepend() { iptables -t nat -C PREROUTING $$* || iptables -t nat -I PREROUTING 1 $$*; }
        ipt_nat_prerouting_prepend -d '${CONTROL_PLANE_HOST}/32' -p tcp --dport 6443 -m tcp -j DNAT --to-destination 127.0.0.1:6443
        ipt_nat_prerouting_prepend -d '${CONTROL_PLANE_HOST}/32' -p tcp --dport 9345 -m tcp -j DNAT --to-destination 127.0.0.1:9345
  preRKE2Commands:
    - (RETRY=60; while ! nslookup get.rke2.io; do ((--RETRY)) || break; sleep 5; done)
    - systemctl enable --now cluster-vip.service
  postRKE2Commands: []
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: ONEMachineTemplate
      name: "${CLUSTER_NAME}-cp"
  serverConfig:
    cloudProviderName: external
    cni: canal
    disableComponents:
      kubernetesComponents:
        - cloudController
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  version: "${KUBERNETES_VERSION:=v1.31.4}+rke2r1"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONEMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      templateName: "${WORKER_TEMPLATE_NAME}"
---
#https://github.com/rancher/cluster-api-provider-rke2/blob/a2e35699da1cd6eb0e0cda66431933f3b602499e/bootstrap/api/v1beta1/rke2configtemplate_types.go#L35
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: RKE2ConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      preRKE2Commands:
        - (RETRY=60; while ! nslookup get.rke2.io; do ((--RETRY)) || break; sleep 5; done)
      agentConfig:
        kubelet:
          #TODO: It would be nice to add cloudProviderName to the agentConfig spec (https://docs.rke2.io/reference/linux_agent_config#cloud-provider)
          extraArgs:
            - --cloud-provider=external
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  replicas: ${WORKER_MACHINE_COUNT:=1}
  clusterName: "${CLUSTER_NAME}"
  selector:
    matchLabels: {}
  template:
    spec:
      clusterName: "${CLUSTER_NAME}"
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: RKE2ConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: ONEMachineTemplate
        name: "${CLUSTER_NAME}-md-0"
      version: "${KUBERNETES_VERSION:=v1.31.4}+rke2r1"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONECluster
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  secretName: "${CLUSTER_NAME}"
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_HOST}"
    port: 6443
  publicNetwork:
    name: "${PUBLIC_NETWORK_NAME}"
  images:
    - imageName: "${MASTER_TEMPLATE_NAME}"
      imageContent: |
        PATH = "https://marketplace.opennebula.io/appliance/4562be1a-4c11-4e9e-b60a-85a045f1de05/download/0"
        DEV_PREFIX = "vd"
  templates:
    - templateName: "${MASTER_TEMPLATE_NAME}"
      templateContent: |
        CONTEXT = [
          BACKEND = "YES",
          NETWORK = "YES",
          GROW_FS = "/",
          SET_HOSTNAME = "$$NAME",
          SSH_PUBLIC_KEY = "$$USER[SSH_PUBLIC_KEY]",
          TOKEN = "YES" ]
        CPU = "1"
        DISK = [
          IMAGE = "${MASTER_TEMPLATE_NAME}",
          SIZE = "${MASTER_DISK_SIZE:-16384}" ]
        GRAPHICS = [
          LISTEN = "0.0.0.0",
          TYPE = "vnc" ]
        HYPERVISOR = "kvm"
        LXD_SECURITY_PRIVILEGED = "true"
        MEMORY = "3072"
        OS = [
          ARCH = "x86_64",
          FIRMWARE_SECURE = "YES" ]
        SCHED_REQUIREMENTS = "HYPERVISOR=kvm"
        VCPU = "2"
    - templateName: "${WORKER_TEMPLATE_NAME}"
      templateContent: |
        CONTEXT = [
          BACKEND = "YES",
          NETWORK = "YES",
          GROW_FS = "/",
          SET_HOSTNAME = "$$NAME",
          SSH_PUBLIC_KEY = "$$USER[SSH_PUBLIC_KEY]",
          TOKEN = "YES" ]
        CPU = "1"
        DISK = [
          IMAGE = "${MASTER_TEMPLATE_NAME}",
          SIZE = "${WORKER_DISK_SIZE:-16384}" ]
        GRAPHICS = [
          LISTEN = "0.0.0.0",
          TYPE = "vnc" ]
        HYPERVISOR = "kvm"
        LXD_SECURITY_PRIVILEGED = "true"
        MEMORY = "3072"
        OS = [
          ARCH = "x86_64",
          FIRMWARE_SECURE = "YES" ]
        SCHED_REQUIREMENTS = "HYPERVISOR=kvm"
        VCPU = "2"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: ONECluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: RKE2ControlPlane
    name: "${CLUSTER_NAME}"
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_HOST}"
    port: 6443
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: "${CLUSTER_NAME}-crs-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
  resources:
    - kind: ConfigMap
      name: cloud-controller-manager
  strategy: Reconcile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cloud-controller-manager
data:
  cloud-controller-manager.yaml: |
    ---
    apiVersion: v1
    kind: Secret
    metadata:
      name: cloud-config
      namespace: kube-system
    stringData:
      config.yaml: |
        opennebula:
          endpoint:
            ONE_XMLRPC: "${ONE_XMLRPC}"
            ONE_AUTH: "${ONE_AUTH}"
          publicNetwork:
            name: "${PUBLIC_NETWORK_NAME}"
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: opennebula-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:opennebula-cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - kind: ServiceAccount
        name: opennebula-cloud-controller-manager
        namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      name: cloud-controller-manager
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: cloud-controller-manager
      template:
        metadata:
          labels:
            k8s-app: cloud-controller-manager
        spec:
          serviceAccountName: opennebula-cloud-controller-manager
          containers:
            - name: cloud-controller-manager
              image: "${CCM_IMG}"
              imagePullPolicy: IfNotPresent
              command:
                - /opennebula-cloud-controller-manager
                - --cloud-provider=opennebula
                - --cluster-name=${CLUSTER_NAME}
                - --cloud-config=/etc/one/config.yaml
                - --leader-elect=true
                - --use-service-account-credentials
                - --controllers=cloud-node,cloud-node-lifecycle
              volumeMounts:
                - name: cloud-config
                  mountPath: /etc/one/
                  readOnly: true
          volumes:
            - name: cloud-config
              secret:
                secretName: cloud-config
          hostNetwork: true
          tolerations:
            - key: node.cloudprovider.kubernetes.io/uninitialized
              value: "true"
              effect: NoSchedule
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
              effect: NoSchedule
            - key: node-role.kubernetes.io/master
              operator: Exists
              effect: NoSchedule
            # TODO: remove this one later!
            - key: node.kubernetes.io/not-ready
              operator: Exists
              effect: NoSchedule
          nodeSelector:
            node-role.kubernetes.io/control-plane: "true"
