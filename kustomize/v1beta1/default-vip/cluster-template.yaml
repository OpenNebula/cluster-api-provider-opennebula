---
apiVersion: v1
kind: Secret
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
stringData:
  ONE_XMLRPC: "${ONE_XMLRPC}"
  ONE_AUTH: "${ONE_AUTH}"
type: Opaque
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONEMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-cp"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      templateName: "${MASTER_TEMPLATE_NAME}"
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
          # NOTE: OpenNebula does not support built-in DNS round-robin (yet).
          kubelet-preferred-address-types: InternalIP,ExternalIP
      controllerManager:
        extraArgs:
          cloud-provider: external
      networking:
        dnsDomain: cluster.local
        serviceSubnet: 10.96.0.0/16
        podSubnet: 10.244.0.0/16
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: external
    files:
      - path: /etc/kubernetes/manifests/kube-vip.yaml
        owner: root:root
        content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
            - args:
              - manager
              env:
              - name: vip_arp
                value: "true"
              - name: port
                value: "6443"
              - name: vip_nodename
                valueFrom:
                  fieldRef:
                    fieldPath: spec.nodeName
              - name: vip_interface
                value: "${VIP_NETWORK_INTERFACE:=eth0}"
              - name: vip_subnet
                value: "32"
              - name: dns_mode
                value: first
              - name: cp_enable
                value: "true"
              - name: cp_namespace
                value: kube-system
              - name: svc_enable
                value: "true"
              - name: svc_leasename
                value: plndr-svcs-lock
              - name: vip_leaderelection
                value: "true"
              - name: vip_leasename
                value: plndr-cp-lock
              - name: vip_leaseduration
                value: "5"
              - name: vip_renewdeadline
                value: "3"
              - name: vip_retryperiod
                value: "1"
              - name: address
                value: "${CONTROL_PLANE_HOST}"
              - name: prometheus_server
                value: :2112
              image: "ghcr.io/kube-vip/kube-vip:${KUBE_VIP_VERSION:=v0.9.0}"
              imagePullPolicy: IfNotPresent
              name: kube-vip
              resources: {}
              securityContext:
                capabilities:
                  add:
                  - NET_ADMIN
                  - NET_RAW
                  drop:
                  - ALL
              volumeMounts:
              - mountPath: /etc/kubernetes/admin.conf
                name: kubeconfig
            hostAliases:
            - hostnames:
              - kubernetes
              ip: 127.0.0.1
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/super-admin.conf
              name: kubeconfig
          status: {}
    preKubeadmCommands:
      - >-
        set -e;
        if ! grep -m1 '^127.0.0.1 kubernetes' /etc/hosts; then
           echo '127.0.0.1 kubernetes' >> /etc/hosts; fi;
    postKubeadmCommands:
      - >-
        set -e;
        if test -d /etc/kubernetes/super-admin.conf; then
           mv /etc/kubernetes/manifests/kube-vip.yaml /var/tmp/;
           sleep 1;
           rmdir /etc/kubernetes/super-admin.conf/ || true;
           kubeadm init phase kubeconfig super-admin || true;
           mv /var/tmp/kube-vip.yaml /etc/kubernetes/manifests/; fi;
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: ONEMachineTemplate
      name: "${CLUSTER_NAME}-cp"
  version: "${KUBERNETES_VERSION:=v1.31.4}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONEMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      templateName: "${WORKER_TEMPLATE_NAME}"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  replicas: ${WORKER_MACHINE_COUNT:=1}
  clusterName: "${CLUSTER_NAME}"
  selector:
    matchLabels: {}
  template:
    spec:
      clusterName: "${CLUSTER_NAME}"
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: ONEMachineTemplate
        name: "${CLUSTER_NAME}-md-0"
      version: "${KUBERNETES_VERSION:=v1.31.4}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: ONECluster
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  secretName: "${CLUSTER_NAME}"
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_HOST}"
    port: 6443
  publicNetwork:
    name: "${PUBLIC_NETWORK_NAME}"
  images:
    - imageName: "${MASTER_TEMPLATE_NAME}"
      imageContent: |
        PATH = "https://d24fmfybwxpuhu.cloudfront.net/capone-6.10.0-3-20250205.qcow2"
        DEV_PREFIX = "vd"
  templates:
    # NOTE: Please escape OpenNebula context variables with additional $ sign,
    #       i.e. "$USER[SSH_PUBLIC_KEY]" becomes "$$USER[SSH_PUBLIC_KEY]".
    - templateName: "${MASTER_TEMPLATE_NAME}"
      templateContent: |
        CONTEXT = [
          BACKEND = "YES",
          NETWORK = "YES",
          GROW_FS = "/",
          SET_HOSTNAME = "$$NAME",
          SSH_PUBLIC_KEY = "$$USER[SSH_PUBLIC_KEY]",
          TOKEN = "YES" ]
        CPU = "1"
        DISK = [
          IMAGE = "${MASTER_TEMPLATE_NAME}",
          SIZE = "${MASTER_DISK_SIZE:-16384}" ]
        GRAPHICS = [
          LISTEN = "0.0.0.0",
          TYPE = "vnc" ]
        HYPERVISOR = "kvm"
        LXD_SECURITY_PRIVILEGED = "true"
        MEMORY = "3072"
        OS = [
          ARCH = "x86_64",
          FIRMWARE_SECURE = "YES" ]
        SCHED_REQUIREMENTS = "HYPERVISOR=kvm"
        VCPU = "2"
    - templateName: "${WORKER_TEMPLATE_NAME}"
      templateContent: |
        CONTEXT = [
          BACKEND = "YES",
          NETWORK = "YES",
          GROW_FS = "/",
          SET_HOSTNAME = "$$NAME",
          SSH_PUBLIC_KEY = "$$USER[SSH_PUBLIC_KEY]",
          TOKEN = "YES" ]
        CPU = "1"
        DISK = [
          IMAGE = "${MASTER_TEMPLATE_NAME}",
          SIZE = "${WORKER_DISK_SIZE:-16384}" ]
        GRAPHICS = [
          LISTEN = "0.0.0.0",
          TYPE = "vnc" ]
        HYPERVISOR = "kvm"
        LXD_SECURITY_PRIVILEGED = "true"
        MEMORY = "3072"
        OS = [
          ARCH = "x86_64",
          FIRMWARE_SECURE = "YES" ]
        SCHED_REQUIREMENTS = "HYPERVISOR=kvm"
        VCPU = "2"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: ONECluster
    name: "${CLUSTER_NAME}"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: "${CLUSTER_NAME}"
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_HOST}"
    port: 6443
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: "${CLUSTER_NAME}-crs-0"
  labels:
    cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: "${CLUSTER_NAME}"
  resources:
    - kind: Secret
      name: "${CLUSTER_NAME}-ccm"
  strategy: Reconcile
---
apiVersion: v1
kind: Secret
metadata:
  name: "${CLUSTER_NAME}-ccm"
type: addons.cluster.x-k8s.io/resource-set
stringData:
  cloud-controller-manager.yaml: |
    ---
    apiVersion: v1
    kind: Secret
    metadata:
      name: cloud-config
      namespace: kube-system
    stringData:
      config.yaml: |
        opennebula:
          endpoint:
            ONE_XMLRPC: "${ONE_XMLRPC}"
            ONE_AUTH: "${ONE_AUTH}"
          publicNetwork:
            name: "${PUBLIC_NETWORK_NAME}"
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: opennebula-cloud-controller-manager
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:opennebula-cloud-controller-manager
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - kind: ServiceAccount
        name: opennebula-cloud-controller-manager
        namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      labels:
        k8s-app: cloud-controller-manager
      name: cloud-controller-manager
      namespace: kube-system
    spec:
      selector:
        matchLabels:
          k8s-app: cloud-controller-manager
      template:
        metadata:
          labels:
            k8s-app: cloud-controller-manager
        spec:
          serviceAccountName: opennebula-cloud-controller-manager
          containers:
            - name: cloud-controller-manager
              image: "${CCM_IMG}"
              imagePullPolicy: IfNotPresent
              command:
                - /opennebula-cloud-controller-manager
                - --cloud-provider=opennebula
                - --cluster-name='${CLUSTER_NAME}'
                - --cloud-config=/etc/one/config.yaml
                - --leader-elect=true
                - --use-service-account-credentials
                - --controllers=cloud-node,cloud-node-lifecycle
              volumeMounts:
                - name: cloud-config
                  mountPath: /etc/one/
                  readOnly: true
          volumes:
            - name: cloud-config
              secret:
                secretName: cloud-config
          hostNetwork: true
          tolerations:
            - key: node.cloudprovider.kubernetes.io/uninitialized
              value: "true"
              effect: NoSchedule
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
              effect: NoSchedule
            - key: node-role.kubernetes.io/master
              operator: Exists
              effect: NoSchedule
            # TODO: remove this one later!
            - key: node.kubernetes.io/not-ready
              operator: Exists
              effect: NoSchedule
          nodeSelector:
            node-role.kubernetes.io/control-plane: ""
